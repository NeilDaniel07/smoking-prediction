# -*- coding: utf-8 -*-
"""SmokingPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qKvyd37oXdUY0TZxo4PudYdqXCmA97l7
"""

!pip install transformers torch

"""#Truncation Technique"""

import pandas as pd
from transformers import AutoTokenizer, BertModel
import torch
from torch.utils.data import TensorDataset
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np

# Load the pre-trained clinicalBERT model

df_train = pd.read_excel("Smoking_Training_Mini.xlsx")
df_test = pd.read_excel("Smoking_Testing_Mini.xlsx")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model_name = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name).to(device)

tokenized_train = tokenizer(df_train["TEXT"].values.tolist(), padding=True, truncation=True, max_length=510, return_tensors="pt")
tokenized_test = tokenizer(df_test["TEXT"].values.tolist(), padding=True, truncation=True, max_length=510, return_tensors="pt")

tokenized_train = {k: torch.tensor(v).to(device) for k, v in tokenized_train.items()}
tokenized_test = {k: torch.tensor(v).to(device) for k, v in tokenized_test.items()}

with torch.no_grad():
    hidden_train = model(**tokenized_train)
    hidden_test = model(**tokenized_test)

cls_train = hidden_train.last_hidden_state[:, 0, :]
cls_test = hidden_test.last_hidden_state[:, 0, :]


x_train = cls_train.to("cpu")
y_train = df_train["STATUS"]

x_test = cls_test.to("cpu")
y_test = df_test["STATUS"]

# Random Forest Classifier
rf = RandomForestClassifier()
rf.fit(x_train, y_train)
rf_score = rf.score(x_test, y_test)
print("Random Forest Classifier Score: ", rf_score)

# Decision Tree Classifier
dt = DecisionTreeClassifier()
dt.fit(x_train, y_train)
dt_score = dt.score(x_test, y_test)
print("Decision Tree Classifier Score: ", dt_score)

# K Nearest Neighbors Classifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
knn_score = knn.score(x_test, y_test)
print("K Nearest Neighbors Classifier Score: ", knn_score)

# Support Vector Machine Classifier
svc = SVC()
svc.fit(x_train, y_train)
svc_score = svc.score(x_test, y_test)
print("Support Vector Machine Classifier Score: ", svc_score)

# Logistic Regression Classifier
lr = LogisticRegression()
lr.fit(x_train, y_train)
lr_score = lr.score(x_test, y_test)
print("Logistic Regression Classifier Score: ", lr_score)

"""# Windowing Technique"""

import pandas as pd
from transformers import AutoTokenizer, BertModel
import torch
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

# Load the pre-trained clinicalBERT model
model_name = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# Load the training and testing data
df_train = pd.read_excel("Smoking_Training.xlsx")
df_test = pd.read_excel("Smoking_Testing.xlsx")

# Tokenize the input sequences and create windows
max_length = 510 #Padding
window_stride = 64

tokenized_train = tokenizer(
    df_train["TEXT"].values.tolist(),
    padding=True,
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)

tokenized_test = tokenizer(
    df_test["TEXT"].values.tolist(),
    padding=True,
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)

windows_train = []
labels_train = []
for input_ids, label in zip(tokenized_train["input_ids"], df_train["STATUS"]):
    input_ids = input_ids.tolist()
    for i in range(0, len(input_ids) - max_length + 1, window_stride):
        window = input_ids[i: i + max_length]
        windows_train.append(window)
        labels_train.append(label)

windows_test = []
labels_test = []
for input_ids, label in zip(tokenized_test["input_ids"], df_test["STATUS"]):
    input_ids = input_ids.tolist()
    for i in range(0, len(input_ids) - max_length + 1, window_stride):
        window = input_ids[i: i + max_length]
        windows_test.append(window)
        labels_test.append(label)

# Convert windows to tensors
input_ids_train = torch.tensor(windows_train)
input_ids_test = torch.tensor(windows_test)

# Generate hidden representations for each window
with torch.no_grad():
    hidden_train = model(input_ids_train)[0]
    hidden_test = model(input_ids_test)[0]

# Average the hidden representations of each window
cls_train = hidden_train.mean(dim=1)
cls_test = hidden_test.mean(dim=1)

# Convert the tensors to CPU
x_train = cls_train.cpu()
y_train = df_train["STATUS"]

x_test = cls_test.cpu()
y_test = df_test["STATUS"]

# Random Forest Classifier
rf = RandomForestClassifier()
rf.fit(x_train, y_train)
rf_score = rf.score(x_test, y_test)
print("Random Forest Classifier Score:", rf_score)

# Decision Tree Classifier
dt = DecisionTreeClassifier()
dt.fit(x_train, y_train)
dt_score = dt.score(x_test, y_test)
print("Decision Tree Classifier Score:", dt_score)

# K Nearest Neighbors Classifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
knn_score = knn.score(x_test, y_test)
print("K Nearest Neighbors Classifier Score:", knn_score)

# Support Vector Machine Classifier
svc = SVC()
svc.fit(x_train, y_train)
svc_score = svc.score(x_test, y_test)
print("Support Vector Machine Classifier Score:", svc_score)

# Logistic Regression Classifier
lr = LogisticRegression()
lr.fit(x_train, y_train)
lr_score = lr.score(x_test, y_test)
print("Logistic Regression Classifier Score:", lr_score)

"""# Chunking Technique - Use The Mean"""

import pandas as pd
from transformers import AutoTokenizer, BertModel
import torch
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

# Load the pre-trained clinicalBERT model
model_name = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# Load the training and testing data
df_train = pd.read_excel("Smoking_Training_Mini.xlsx")
df_test = pd.read_excel("Smoking_Testing_Mini.xlsx")

# Tokenize the input sequences
max_length = 510
chunk_length = 510
stride = 510

def split_into_chunks(input_ids):
    chunks = []
    for i in range(0, len(input_ids), stride):
        chunk = input_ids[i : i + chunk_length]
        chunks.append(chunk)
    return chunks

tokenized_train = tokenizer(
    df_train["TEXT"].values.tolist(),
    padding=True,
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)

tokenized_test = tokenizer(
    df_test["TEXT"].values.tolist(),
    padding=True,
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)

chunks_train = []
labels_train = []
for input_ids, label in zip(tokenized_train["input_ids"], df_train["STATUS"]):
    input_ids = input_ids.tolist()
    chunks = split_into_chunks(input_ids)
    chunks_train.extend(chunks)
    labels_train.extend([label] * len(chunks))

chunks_test = []
labels_test = []
for input_ids, label in zip(tokenized_test["input_ids"], df_test["STATUS"]):
    input_ids = input_ids.tolist()
    chunks = split_into_chunks(input_ids)
    chunks_test.extend(chunks)
    labels_test.extend([label] * len(chunks))

# Convert chunks to tensors
input_ids_train = torch.tensor(chunks_train)
input_ids_test = torch.tensor(chunks_test)

# Generate hidden representations for each chunk
with torch.no_grad():
    hidden_train = model(input_ids_train)[0]
    hidden_test = model(input_ids_test)[0]

# Average the hidden representations of each chunk
cls_train = hidden_train.mean(dim=1)
cls_test = hidden_test.mean(dim=1)

# Convert the tensors to CPU
x_train = cls_train.cpu()
y_train = labels_train

x_test = cls_test.cpu()
y_test = labels_test

# Random Forest Classifier
rf = RandomForestClassifier()
rf.fit(x_train, y_train)
rf_score = rf.score(x_test, y_test)
print("Random Forest Classifier Score:", rf_score)

# Decision Tree Classifier
dt = DecisionTreeClassifier()
dt.fit(x_train, y_train)
dt_score = dt.score(x_test, y_test)
print("Decision Tree Classifier Score:", dt_score)

# K Nearest Neighbors Classifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
knn_score = knn.score(x_test, y_test)
print("K Nearest Neighbors Classifier Score:", knn_score)

# Support Vector Machine Classifier
svc = SVC()
svc.fit(x_train, y_train)
svc_score = svc.score(x_test, y_test)
print("Support Vector Machine Classifier Score:", svc_score)

# Logistic Regression Classifier
lr = LogisticRegression()
lr.fit(x_train, y_train)
lr_score = lr.score(x_test, y_test)
print("Logistic Regression Classifier Score:", lr_score)

"""# Chunking Technique - Use The Sum"""

import pandas as pd
from transformers import AutoTokenizer, BertModel
import torch
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

# Load the pre-trained clinicalBERT model
model_name = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# Load the training and testing data
df_train = pd.read_excel("Smoking_Training_Mini.xlsx")
df_test = pd.read_excel("Smoking_Testing_Mini.xlsx")

# Tokenize the input sequences and create windows
max_length = 510  # Padding
window_stride = 64

tokenized_train = tokenizer(
    df_train["TEXT"].values.tolist(),
    padding=True,
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)

tokenized_test = tokenizer(
    df_test["TEXT"].values.tolist(),
    padding=True,
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)

chunks_train = []
labels_train = []
for input_ids, label in zip(tokenized_train["input_ids"], df_train["STATUS"]):
    input_ids = input_ids.tolist()
    for i in range(0, len(input_ids) - max_length + 1, window_stride):
        chunk = input_ids[i: i + max_length]
        chunks_train.append(chunk)
        labels_train.append(label)

chunks_test = []
labels_test = []
for input_ids, label in zip(tokenized_test["input_ids"], df_test["STATUS"]):
    input_ids = input_ids.tolist()
    for i in range(0, len(input_ids) - max_length + 1, window_stride):
        chunk = input_ids[i: i + max_length]
        chunks_test.append(chunk)
        labels_test.append(label)

# Convert chunks to tensors
input_ids_train = torch.tensor(chunks_train)
input_ids_test = torch.tensor(chunks_test)

# Generate hidden representations for each chunk
with torch.no_grad():
    hidden_train = model(input_ids_train)[0]
    hidden_test = model(input_ids_test)[0]

# Sum the hidden representations of each chunk
cls_train = hidden_train.sum(dim=1)
cls_test = hidden_test.sum(dim=1)

# Convert the tensors to CPU
x_train = cls_train.cpu()
y_train = df_train["STATUS"]

x_test = cls_test.cpu()
y_test = df_test["STATUS"]

# Random Forest Classifier
rf = RandomForestClassifier()
rf.fit(x_train, y_train)
rf_score = rf.score(x_test, y_test)
print("Random Forest Classifier Score:", rf_score)

# Decision Tree Classifier
dt = DecisionTreeClassifier()
dt.fit(x_train, y_train)
dt_score = dt.score(x_test, y_test)
print("Decision Tree Classifier Score:", dt_score)

# K Nearest Neighbors Classifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
knn_score = knn.score(x_test, y_test)
print("K Nearest Neighbors Classifier Score:", knn_score)

# Support Vector Machine Classifier
svc = SVC()
svc.fit(x_train, y_train)
svc_score = svc.score(x_test, y_test)
print("Support Vector Machine Classifier Score:", svc_score)

# Logistic Regression Classifier
lr = LogisticRegression()
lr.fit(x_train, y_train)
lr_score = lr.score(x_test, y_test)
print("Logistic Regression Classifier Score:", lr_score)

"""# Playground"""

import pandas as pd
from transformers import AutoTokenizer, BertModel
import torch
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
import math
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
plt.style.use("fivethirtyeight")

# Load the pre-trained clinicalBERT model
model_name = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# Load the training and testing data
df_train = pd.read_excel("Smoking_Training_Mini.xlsx")
df_test = pd.read_excel("Smoking_Testing_Mini.xlsx")

# Tokenize the input sequences and create windows
max_length = 510  # Padding
window_stride = 64

tokenized_train = tokenizer(
    df_train["TEXT"].values.tolist(),
    padding=True,
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)

tokenized_test = tokenizer(
    df_test["TEXT"].values.tolist(),
    padding=True,
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)

chunks_train = []
labels_train = []
for input_ids, label in zip(tokenized_train["input_ids"], df_train["STATUS"]):
    input_ids = input_ids.tolist()
    for i in range(0, len(input_ids) - max_length + 1, window_stride):
        chunk = input_ids[i: i + max_length]
        chunks_train.append(chunk)
        labels_train.append(label)

chunks_test = []
labels_test = []
for input_ids, label in zip(tokenized_test["input_ids"], df_test["STATUS"]):
    input_ids = input_ids.tolist()
    for i in range(0, len(input_ids) - max_length + 1, window_stride):
        chunk = input_ids[i: i + max_length]
        chunks_test.append(chunk)
        labels_test.append(label)

# Convert chunks to tensors
input_ids_train = torch.tensor(chunks_train)
input_ids_test = torch.tensor(chunks_test)

# Generate hidden representations for each chunk
with torch.no_grad():
    hidden_train = model(input_ids_train)[0]
    hidden_test = model(input_ids_test)[0]

# Sum the hidden representations of each chunk
cls_train = hidden_train.sum(dim=1)
cls_test = hidden_test.sum(dim=1)

# Convert the tensors to CPU
x_train = cls_train.cpu()
y_train = df_train["STATUS"]

x_test = cls_test.cpu()
y_test = df_test["STATUS"]

scaler = StandardScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

# Random Forest Classifier
rf = RandomForestClassifier()
rf.fit(x_train, y_train)
rf_score = rf.score(x_test, y_test)
print("Random Forest Classifier Score:", rf_score)

# Decision Tree Classifier
dt = DecisionTreeClassifier()
dt.fit(x_train, y_train)
dt_score = dt.score(x_test, y_test)
print("Decision Tree Classifier Score:", dt_score)

# K Nearest Neighbors Classifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
knn_score = knn.score(x_test, y_test)
print("K Nearest Neighbors Classifier Score:", knn_score)

# Support Vector Machine Classifier
svc = SVC()
svc.fit(x_train, y_train)
svc_score = svc.score(x_test, y_test)
print("Support Vector Machine Classifier Score:", svc_score)

# Logistic Regression Classifier
lr = LogisticRegression()
lr.fit(x_train, y_train)
lr_score = lr.score(x_test, y_test)
print("Logistic Regression Classifier Score:", lr_score)

#Neural Network - LBFGS
clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(400, 200, 100), random_state=1, learning_rate_init=0.01)
clf.fit(x_train, y_train)
clf_score = clf.score(x_test, y_test)
print("Neural Network Score (LGBFS Solver): ", clf_score)

#Neural Network - adam
clf2 = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(400, 200, 100), random_state=1, learning_rate_init=0.01)
clf2.fit(x_train, y_train)
clf2_score = clf2.score(x_test, y_test)
print("Neural Network Score (Adam Solver): ", clf2_score)

#Neural Network - SGD
clf3 = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(400, 200, 100), random_state=1, learning_rate_init=0.01)
clf3.fit(x_train, y_train)
clf3_score = clf3.score(x_test, y_test)
print("Neural Network Score (SGD Solver): ", clf3_score)

from keras.layers import Dropout
model = Sequential()
model.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))
model.add(Dropout(0.1))
model.add(LSTM(units = 50, return_sequences = True))
model.add(Dropout(0.1))
model.add(LSTM(units = 50, return_sequences = True))
model.add(Dropout(0.1))
model.add(LSTM(units = 50))
model.add(Dropout(0.1))
model.add(Dense(units = 1))

model.compile(optimizer = "adam", loss = "mean_squared_error")
model.fit(x_train, y_train, epochs = 2, batch_size = 50)
lstm_score = model.score(x_test, y_test)
print("LSTM Score: ", lstm_score)